<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-09-24 Tue 18:00 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Bidipta Sarkar</title>
<meta name="author" content="Bidipta Sarkar" />
<meta name="description" content="Bidipta Sarkar's Personal Homepage" />
<meta name="keywords" content="homepage, website, research, AI, RL, MARL, Vision, Graphics" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="home_style.css"/>
<link rel="stylesheet" type="text/css" href="style.css"/>
<script src="https://kit.fontawesome.com/1eb1a53221.js" crossorigin="anonymous"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
<script src="common_animations.js"></script>
<link rel="icon" type="image/x-icon" href="favicon.ico">
</head>
<body>
<div id="content" class="content">

<div class="page-container">
<div class="topsection">

<div id="outline-container-titlebar-head">
<h2 id="titlebar-head">Bidipta Sarkar</h2>
<div id="nav-pages">
<input class="menu-btn" type="checkbox" id="menu-btn" /> <label class="menu-icon" for="menu-btn"><span class="navicon"></span></label>
<ul class="org-ul">

<li><a href="index.html">Home</a></li>
            
<li><a href="research/index.html">Research</a></li>
	    
<li><a href="art/index.html">Art Gallery</a></li>
            
<li><a href="blog/index.html">Blog</a></li>
</ul>
</div>
</div>

<div class="sidebar">
    <div id="sidebar-head">
        <span class="image">
            <img src="Bidipta_Sarkar_600_600.jpg" style="border-radius: 50%">
        </span>
    </div>
    <nav id="sidebar-nav">
        <div id="nav-icons">
            <ul>
                
                    <i><a href="mailto:bidipta.sarkar@eng.ox.ac.uk" class="fa fa-envelope"></a></i>
                
                    <i><a href="https://github.com/bsarkar321" class="fa fa-github"></a></i>
                
                    <i><a href="https://scholar.google.com/citations?user=wr9RgmcAAAAJ" class="ai ai-google-scholar-square"></a></i>
                
                    <!-- <i><a href="https://www.instagram.com/bidiptas13/" class="fa fa-instagram"></a></i> -->

		    <i><a href="https://twitter.com/bidiptas13" class="fa fa-twitter"></a></i>

		    <i><a href="https://youtube.com/@bidi_2" class="fa fa-youtube"></a></i>

            </ul>
        </div>    
    </nav>
</div>

<div class="content_inner">
        <section id="home">
	  <div class="container">

<p>
I am a first-year DPhil student in Engineering Science at the <a href="https://eng.ox.ac.uk/">University of Oxford</a> as a member of Professor <a href="https://www.jakobfoerster.com/">Jakob Foerster</a>'s <a href="https://foersterlab.com/">FLAIR</a> lab.<br />
</p>

<p>
I received my BS in Computer Science at <a href="https://cs.stanford.edu">Stanford University</a> (2020-2024), where I was also a member of Professor <a href="https://dorsa.fyi">Dorsa Sadigh</a>'s <a href="https://iliad.stanford.edu/">ILIAD</a> lab since my sophomore year.<br />
</p>

<p>
I am interested in creating AI agents that can interact with their environment and safely work alongside humans and other autonomous agents. My research broadly spans three subfields of computer science:<br />
</p>

<p>
&bull; Multi-Agent Reinforcement Learning: Enabling independently trained agents to cooperate on a common task and form conventions.<br />
</p>

<p>
&bull; Vision: Capturing meaningful information about an agent's environment from sensors.<br />
</p>

<p>
&bull; Graphics: Simulating environments while balancing speed and realism.<br />
</p>

</div></section></div></div><hr>
<div id="outline-container-blog-post" class="outline-2">
<h2 id="blog-post">Posts</h2>
<div class="outline-text-2" id="text-blog-post">
</div>
<div id="outline-container-org3506467" class="outline-3">
<h3 id="org3506467">(November 2023, Video) <a href="https://youtu.be/wm4f0sdKIUA">My talk on Diverse Conventions for Human-AI Collaboration</a></h3>
</div>

<div id="outline-container-org8ca5c23" class="outline-3">
<h3 id="org8ca5c23">(August 2023, Blog Post) <a href="blog/overcooked_madrona/index.html">Overcooked in Thousands of Kitchens: Training Top Performing Agents in Under a Minute</a></h3>
</div>

<div id="outline-container-org6ea79f6" class="outline-3">
<h3 id="org6ea79f6">(Living Document) <a href="https://bsarkar321.github.io/emacs_setup/">Emacs Setup for macOS and GNU/Linux</a></h3>
<div class="outline-text-3" id="text-org6ea79f6">
<hr>
</div>
</div>
</div>
<div id="outline-container-org9b81951" class="outline-2">
<h2 id="org9b81951">Publications</h2>
<div class="outline-text-2" id="text-org9b81951">
</div>
<div id="outline-container-org81666f0" class="outline-3">
<h3 id="org81666f0">Physically Grounded Vision-Language Models for Robotic Manipulation</h3>
<div class="outline-text-3" id="text-org81666f0">
<div class="outline-text-3-inner">
<p>
Jensen Gao, <span class="underline">Bidipta Sarkar</span>, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh<br />
</p>

<p>
<i>International Conference on Robotics and Automation (ICRA), May 2024</i><br />
</p>

<p>
<a href="https://arxiv.org/abs/2309.02561">Paper</a> / <a href="https://iliad.stanford.edu/pg-vlm/">Website</a> / <a href="https://drive.google.com/file/d/17gbzrJSs8YjVafIrX4omR_rx6qLgXjUd/view">Video</a><br />
</p>
</div>


<div id="org119aae7" class="figure">
<p><img src="old_reports/vlm_image.png" alt="vlm_image.png" /><br />
</p>
</div>
</div>
</div>
<div id="outline-container-org99ec982" class="outline-3">
<h3 id="org99ec982">Diverse Conventions for Human-AI Collaboration</h3>
<div class="outline-text-3" id="text-org99ec982">
<div class="outline-text-3-inner">
<p>
<span class="underline">Bidipta Sarkar</span>, Andy Shih, Dorsa Sadigh<br />
</p>

<p>
<i>Conference on Neural Information Processing Systems (NeurIPS), December 2023</i><br />
</p>

<p>
<a href="https://arxiv.org/abs/2310.15414">Paper</a> / <a href="https://iliad.stanford.edu/Diverse-Conventions/">Website</a> / <a href="https://github.com/Stanford-ILIAD/Diverse-Conventions">Code</a> / <a href="https://youtu.be/wm4f0sdKIUA">Video</a><br />
</p>
</div>


<div id="orgadcd1fa" class="figure">
<p><img src="old_reports/XPHandshake.png" alt="XPHandshake.png" /><br />
</p>
</div>
</div>
</div>
<div id="outline-container-org1c19316" class="outline-3">
<h3 id="org1c19316">An Extensible, Data-Oriented Architecture for High-Performance, Many-World Simulation</h3>
<div class="outline-text-3" id="text-org1c19316">
<div class="outline-text-3-inner">
<p>
Brennan Shacklett, Luc Guy Rosenzweig, Zhiqiang Xie, <span class="underline">Bidipta Sarkar</span>, Andrew Szot, Erik Wijmans, Vladlen Koltun, Dhruv Batra, Kayvon Fatahalian<br />
</p>

<p>
<i>Transactions on Graphics 2023</i><br />
</p>

<p>
<a href="https://madrona-engine.github.io/shacklett_siggraph23.pdf">Paper</a> / <a href="https://madrona-engine.github.io">Website</a> / <a href="https://github.com/bsarkar321/madrona_rl_envs">RL Environments</a> / <a href="file:///Users/bidiptasarkar/Desktop/iCloudDrive/personal_website/blog/overcooked_madrona/index.html">Blog</a> / <a href="https://colab.research.google.com/github/bsarkar321/madrona_rl_envs/blob/master/overcooked_compiled_colab.ipynb">Colab</a><br />
</p>
</div>


<div id="org0ace2b8" class="figure">
<p><img src="old_reports/madrona.png" alt="madrona.png" /><br />
</p>
</div>
</div>
</div>
<div id="outline-container-org7b5be39" class="outline-3">
<h3 id="org7b5be39">PantheonRL: A MARL Library for Dynamic Training Interactions</h3>
<div class="outline-text-3" id="text-org7b5be39">
<div class="outline-text-3-inner">
<p>
<span class="underline">Bidipta Sarkar*</span>, Aditi Talati*, Andy Shih*, Dorsa Sadigh<br />
</p>

<p>
<i>Proceedings of the 36th AAAI Conference on Artificial Intelligence (Demo Track), February 2022</i><br />
</p>

<p>
<a href="https://arxiv.org/abs/2112.07013">Paper</a> / <a href="https://github.com/Stanford-ILIAD/PantheonRL">Code</a> / <a href="https://youtu.be/3-Pf3zh_Hpo">Video</a> / <a href="https://doi.org/10.1609/aaai.v36i11.21734">DOI</a><br />
</p>
</div>


<div id="org58e17b3" class="figure">
<p><img src="old_reports/round_robin.png" alt="round_robin.png" /><br />
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org8ef810d" class="outline-2">
<h2 id="org8ef810d">Bachelor's Honors Thesis</h2>
<div class="outline-text-2" id="text-org8ef810d">
</div>
<div id="outline-container-org9167434" class="outline-3">
<h3 id="org9167434">Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning</h3>
<div class="outline-text-3" id="text-org9167434">
<div class="outline-text-3-inner">
<p>
<span class="underline">Bidipta Sarkar</span>, Warren Xia, C. Karen Liu, Dorsa Sadigh<br />
</p>

<p>
<i>Stanford Digital Repository, May 2024</i><br />
</p>

<p>
<a href="https://doi.org/10.25740/fr666rz8034">Paper</a> / <a href="https://github.com/SocialDeductionLLM">Website</a> / <a href="https://github.com/SocialDeductionLLM">Code</a> / <a href="https://doi.org/10.25740/fr666rz8034">DOI</a><br />
</p>
</div>


<div id="org9d9e7ac" class="figure">
<p><img src="old_reports/AmongUsDiagrams.png" alt="AmongUsDiagrams.png" /><br />
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org0a00be1" class="outline-2">
<h2 id="org0a00be1">Preprints</h2>
<div class="outline-text-2" id="text-org0a00be1">
</div>
<div id="outline-container-org3bce674" class="outline-3">
<h3 id="org3bce674">An Interactive Agent Foundation Model</h3>
<div class="outline-text-3" id="text-org3bce674">
<div class="outline-text-3-inner">
<p>
Zane Durante*, <span class="underline">Bidipta Sarkar*</span>, Ran Gong*, Rohan Taori, Yusuke Noda, Paul Tang, Ehsan Adeli, Shrinidhi Kowshika Lakshmikanth, Kevin Schulman, Arnold Milstein, Demetri Terzopoulos, Ade Famoti, Noboru Kuno, Ashley Llorens, Hoi Vo, Katsu Ikeuchi, Li Fei-Fei, Jianfeng Gao, Naoki Wake*, Qiuyuan Huang*<br />
</p>

<p>
<i>arXiv, February 2024</i><br />
</p>

<p>
<a href="https://arxiv.org/abs/2402.05929">Paper</a> / <a href="https://agentfoundationmodel.github.io/">Website</a><br />
</p>
</div>


<div id="org0912788" class="figure">
<p><img src="old_reports/Agentframework.png" alt="Agentframework.png" /><br />
</p>
</div>

<hr>
</div>
</div>
</div>
<div id="outline-container-org873d3b3" class="outline-2">
<h2 id="org873d3b3">Other Projects</h2>
<div class="outline-text-2" id="text-org873d3b3">
</div>
<div id="outline-container-org4207448" class="outline-3">
<h3 id="org4207448">Temporally and Spatially Novel Video Frame Synthesis using 4D Video Autoencoder</h3>
<div class="outline-text-3" id="text-org4207448">
<div class="outline-text-3-inner">
<p>
<span class="underline">Bidipta Sarkar</span>, Xinyi Wang, Kathy Yu<br />
</p>

<p>
<i>CS231n Final Project, Spring 2022</i><br />
</p>

<p>
(Best Project Poster Award)
</p>

<p>
<a href="http://cs231n.stanford.edu/reports/2022/pdfs/8.pdf">Report</a> / <a href="http://cs231n.stanford.edu/reports/2022/pdfs/8p.pdf">Poster</a> / <a href="https://twitter.com/drfeifei/status/1533840002735407104?s=46&amp;t=M7k1cW-r1qVcTdl16BlH-A">CS231n Tweet</a> / <a href="https://github.com/KathyFeiyang/cs231n-project">Code</a><br />
</p>
</div>


<div id="orgb292ac2" class="figure">
<p><img src="old_reports/4dencoder.png" alt="4dencoder.png" /><br />
</p>
</div>
</div>
</div>
<div id="outline-container-orge29ec6d" class="outline-3">
<h3 id="orge29ec6d">Simulating Food Interactions with Material Point Methods in Houdini</h3>
<div class="outline-text-3" id="text-orge29ec6d">
<div class="outline-text-3-inner">
<p>
<i>CS348C Final Project, Winter 2022</i><br />
</p>

<p>
<a href="old_reports/348c/bidipta_mpm.txt">Report</a> / <a href="old_reports/348c/mpm_houdini.hipnc">Houdini File</a> / <a href="old_reports/348c/mpm_houdini.mantra_ipr.mov">Video</a><br />
</p>
</div>


<div id="orgc750831" class="figure">
<p><img src="old_reports/348c_img.png" alt="348c_img.png" /><br />
</p>
</div>
</div>
</div>
<div id="outline-container-org76ec27e" class="outline-3">
<h3 id="org76ec27e">Real-time Cel Shading</h3>
<div class="outline-text-3" id="text-org76ec27e">
<div class="outline-text-3-inner">
<p>
<i>CS248 Final Project, Winter 2022</i><br />
</p>

<p>
<a href="old_reports/248/writeup.pdf">Report</a> / <a href="https://drive.google.com/file/d/1RBGs3V28lLoKfwNxFO9VhEpRoYSzle0C/view?usp=sharing">Video</a><br />
</p>
</div>


<div id="orge5efaa0" class="figure">
<p><img src="old_reports/cel_shading.png" alt="cel_shading.png" /><br />
</p>
</div>
</div>
</div>
<div id="outline-container-orgd252164" class="outline-3">
<h3 id="orgd252164">Virtual Hand Interactions with ARKit</h3>
<div class="outline-text-3" id="text-orgd252164">
<div class="outline-text-3-inner">
<p>
<i>CS231a Final Project, Winter 2022</i><br />
</p>

<p>
<a href="old_reports/231a.pdf">Report</a> / <a href="https://github.com/bsarkar321/ARHandInteraction">Code</a> / <a href="https://drive.google.com/file/d/1Rh1sHnOpdZxG6oVDIftx-JYUXNY7n2GF/view?usp=sharing">Demo Video</a><br />
</p>
</div>


<div id="org9778201" class="figure">
<p><img src="old_reports/ar_hands.png" alt="ar_hands.png" /><br />
</p>
</div>
</div>
</div>
<div id="outline-container-org16db439" class="outline-3">
<h3 id="org16db439">The Guardian</h3>
<div class="outline-text-3" id="text-org16db439">
<div class="outline-text-3-inner">
<p>
<i>CS148 Final Project, Fall 2022</i><br />
</p>

<p>
<a href="old_reports/148/bidiptas.pdf">Report</a> / <a href="old_reports/148/bidiptas.png">Final Image</a> / <a href="old_reports/148/bidiptas_a.png">View 2</a> / <a href="old_reports/148/bidiptas_b.png">No Material</a> / <a href="https://drive.google.com/drive/folders/1lzNt7bS1suecTQy8GmH74S8LIcLHL-1y">Blend File</a><br />
</p>
</div>


<div id="orge345b07" class="figure">
<p><img src="old_reports/148/bidiptas.png" alt="bidiptas.png" /><br />
</p>
</div>
</div>
</div>
<div id="outline-container-org01b351d" class="outline-3">
<h3 id="org01b351d">Multi-Agent Self-Learning Tank Game</h3>
<div class="outline-text-3" id="text-org01b351d">
<div class="outline-text-3-inner">
<p>
<span class="underline">Bidipta Sarkar</span>, Henry Ang<br />
</p>

<p>
<i>CS221 Final Project, Spring 2021</i><br />
</p>

<p>
<a href="old_reports/221.pdf">Report</a> / <a href="https://github.com/guohaoang/TankGym">Code</a> / <a href="https://youtu.be/kcZmCApSudY">Video</a><br />
</p>
</div>


<div id="org79d9191" class="figure">
<p><img src="old_reports/221.jpeg" alt="221.jpeg" /><br />
</p>
</div>

<hr>
</div>
</div>
</div>
<div id="outline-container-org067e3aa" class="outline-2">
<h2 id="org067e3aa">Teaching and Outreach</h2>
<div class="outline-text-2" id="text-org067e3aa">
</div>
<div id="outline-container-orgb25abe6" class="outline-3">
<h3 id="orgb25abe6">Stanford Center for Teaching and Learning CS Tutor</h3>
<div class="outline-text-3" id="text-orgb25abe6">

<div id="org3cdbbbd" class="figure">
<p><img src="old_reports/CTL.png" alt="CTL.png" /><br />
</p>
</div>
</div>
</div>
<div id="outline-container-org1a7fee8" class="outline-3">
<h3 id="org1a7fee8">Tau Beta Pi Mentor</h3>
<div class="outline-text-3" id="text-org1a7fee8">

<div id="org148202a" class="figure">
<p><img src="old_reports/TBP.png" alt="TBP.png" /><br />
</p>
</div>
</div>
</div>
<div id="outline-container-orgc736254" class="outline-3">
<h3 id="orgc736254">Section Leader (CS 106A and 106B)</h3>
<div class="outline-text-3" id="text-orgc736254">

<div id="org6db6b7c" class="figure">
<p><img src="old_reports/198.png" alt="198.png" height="100 em" /><br />
</p>
</div>

<hr>

        <div class="footer">
	  <p id="copyright">
            &copy; 2023 Bidipta Sarkar
	  </p>
	</div>
</div>
</div>
</div>
</div>
</div>
</body>
</html>
