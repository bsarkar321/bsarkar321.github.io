<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-11-09 Sun 21:47 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Evolution Strategies at the Hyperscale</title>
<meta name="author" content="Bidipta Sarkar" />
<meta name="description" content="Blog post for ES at the Hyperscale" />
<meta name="keywords" content="homepage, website, research, AI, RL, MARL, Vision, Graphics" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="./style.css"/>
<link rel="stylesheet" type="text/css" href="./bformat.css"/>
<script src="https://kit.fontawesome.com/1eb1a53221.js" crossorigin="anonymous"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
<link rel="icon" type="image/x-icon" href="../../favicon.ico">
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<div class="page-container">

<script>
  function toggleCollapse(c) {
    console.log("WOW");
    c.classList.toggle("active");
    console.log("WOW2");
    var content = c.nextElementSibling;
    console.log(content);
    if (content.style.maxHeight){
      content.style.maxHeight = null;
    } else {
      content.style.maxHeight = content.scrollHeight + 20 + "px";
    }
  }
</script>



<div><div><div><div><div class="content_inner"><section id="home"></section></div></div>
<div id="outline-container-org0182930" class="outline-2">
<h2 id="org0182930"><b>Evolution Strategies at the Hyperscale</b></h2>
<div class="outline-text-2" id="text-org0182930">
<p class="c48">General ML Training Made as Fast and Easy as Inference</p><hr>

<p>
<i>Written by: <a href="https://bsarkar321.github.io/">Bidipta Sarkar</a></i><br />
<i>Edited by: TODO</i><br />
</p>

<img src="throughput.png" style="width:80%;image-rendering:auto;">

<p>
We are excited to introduce EGGROLL, a general-purpose algorithm for training ML systems (including LLMs) that matches the speed and resource requirements of batched inference, giving us 3x the speed of RL. EGGROLL is a variant of <a href="https://lilianweng.github.io/posts/2019-09-05-evolution-strategies/">Evolution Strategies</a> that uses batched LoRA inference to generate full-rank parameter updates, and we empirically find that it is comparable to full-parameter evolution per update while being orders of magnitude faster at large population sizes.<br />
</p>

<p>
EGGROLL effectively <b>eliminates</b> the barrier between inference and training. In particular, <b>any system for high-throughput batched LoRA inference can now be converted into a system for high-throughput general neural network training</b>. In addition to standard ML settings, like supervised learning or reinforcement learning, this enables us to train networks that are aware of complex inference-time systems, such as multi-agent LLM systems or complex samplers where reinforcement learning would struggle.<br />
</p>

<p>
In the rest of this blog post, we describe the EGGROLL algorithm and show how it performs in classic ES settings (tabula-rasa game learning) and common LLM "reasoning" settings. We will also show preliminary experiments of high-throughput pretraining of a nonlinear RNN language model, where all the weights are in int8 and floating-point operations are never used, a task that is only made practical by EGGROLL.<br />
</p>


<blockquote>
<p>
TL;DR: We introduce a new evolution-based method called EGGROLL. We show that:<br />
</p>
<div class="step2bullets">
<ul class="org-ul">
<li>In classic tabula-rasa game learning with small networks, EGGROLL does not compromise performance relative to standard ES, matching the method in <a href="https://arxiv.org/abs/1703.03864">OpenAI's Evolution Strategies paper</a> on a single GPU.<br /></li>
<li>On LLM "reasoning" settings EGGROLL gets comparable results to <a href="https://arxiv.org/abs/2509.24372">concurrent work on naive ES on LLMs</a> <sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup> on the countdown task using the RWKV-7 language model.<br /></li>
<li>EGGROLL can stably train a nonlinear RNN language model, where the weights are in int8 and all operations are done on integers.<br /></li>
</ul>
</div>

<p>
To accompany this blog post, we also release:<br />
</p>
<div class="step2bullets">
<ul class="org-ul">
<li>A jax-based experimental neural network library to use EGGROLL on custom settings, along with starter code for training the RWKV language model.<br /></li>
<li>A single-file implementation of pure int8 language model training. We highly encourage community contributions, similar to the <a href="https://github.com/KellerJordan/modded-nanogpt/tree/master">nanogpt speedrun</a>, to see how efficient we can make pure evolution pretraining in integer formats!<br /></li>
</ul>
</div>

<p>
This is an early research checkpoint of our work. We will continue iterating on our methods and results, but we wanted to share these results early with the community to get feedback and more thoughts.<br />
</p>
</blockquote>
</div>
</div>
<div id="outline-container-orgb72affc" class="outline-2">
<h2 id="orgb72affc">What is EGGROLL</h2>
<div class="outline-text-2" id="text-orgb72affc">
<p><b>EGGROLL</b> stands for <b>E</b>volution <b>G</b>uided <b>G</b>roup <b>R</b>elative <b>O</b>nline <b>L</b>ow-rank <b>L</b>earning.</p>

<p>
To explain each part of the acronym, we will first give a brief overview on how <b>Evolution Strategies</b> works; for a more complete description of the history and variants, we highly recommend <a href="https://lilianweng.github.io/posts/2019-09-05-evolution-strategies/">Lilian Weng's blog post on ES</a>.<br />
</p>

<p>
Evolution Strategies work by directly optimizing the parameters of a neural network by sampling random perturbations and shifting the parameters towards the perturbations that give the best fitness. Mathematically, OpenAI's ES formulation is represented as:<br />
</p>

<p>
\[\nabla_{\theta}\mathbb{E}_{\epsilon\sim N(0,I)} F(\theta+\sigma\epsilon) = \frac{1}{\sigma}\mathbb{E}_{\epsilon\sim N(0,I)}\{F(\theta+\sigma\epsilon)\epsilon\}\]<br />
</p>

<p>
where \(F\) is the fitness function, which measures how good a specific set of parameters are at the task at hand similar to the reward function in RL, \(\theta\) are the parameters you are optimizing, and \(\sigma\) is the standard deviation of the noise to add to the parameters. Typically the fitness function is not the raw rewards, since that would be sensitive to the scale of rewards, so we instead borrow the <b>Group Relative</b> normalization from <a href="https://arxiv.org/abs/2503.20783">DR GRPO</a>.<br />
</p>

<p>
In OpenAI's Evolution Strategies, we sample from a normal distribution independently for each parameter. In jax, this can be represented as follows for a standard matrix multiplication, where thread_id is the index of the population member you are evaluating:<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #C678DD;">def</span> <span style="color: #61AFEF;">forward</span>(base_perturbation_key, sigma, parameter, x, thread_id):
<span class="linenr">2: </span>    <span style="color: #E06C75;">key</span> = jax.random.fold_in(base_perturbation_key, thread_id)
<span class="linenr">3: </span>    <span style="color: #E06C75;">perturbation</span> = jax.random.normal(key, parameter.shape) * sigma
<span class="linenr">4: </span>    <span style="color: #C678DD;">return</span> x @ (parameter + perturbation).T
<span class="linenr">5: </span>
<span class="linenr">6: </span><span style="color: #E06C75;">batch_forward</span> = jax.vmap(forward, in_axes=(<span style="color: #56B6C2;">None</span>, <span style="color: #56B6C2;">None</span>, <span style="color: #56B6C2;">None</span>, 0, 0))
</pre>
</div>

<p>
Note that standard matrix multiplication has now turned into a batched matrix multiplication, which is extremely inefficient on GPUs for large matrices and large populations.<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup><br />
</p>

<p>
Our approach is instead to structure our perturbations to explicitly be low-rank. This enables us to do a large standard matrix multiplication, alongside a batched vector-vector multiplication and batched scalar-vector multipliation at rank 1. This is extremely fast and scalable, giving us the throughput curves in the headline image (less than 20% slower than pure non-lora inference). In jax, this is represented as:<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #C678DD;">def</span> <span style="color: #61AFEF;">forward</span>(base_perturbation_key, sigma, parameter, x, thread_id, rank=1):
<span class="linenr">2: </span>    <span style="color: #E06C75;">key</span> = jax.random.fold_in(base_perturbation_key, thread_id)
<span class="linenr">3: </span>    <span style="color: #E06C75;">a</span>, <span style="color: #E06C75;">b</span> = parameter.shape
<span class="linenr">4: </span>    <span style="color: #E06C75;">perturbation</span> = jax.random.normal(key, (a+b, rank))
<span class="linenr">5: </span>    <span style="color: #E06C75;">B</span> = lora_params[:b]  <span style="color: #5C6370; font-style: italic;"># </span><span style="color: #5C6370; font-style: italic;">b x r</span>
<span class="linenr">6: </span>    <span style="color: #E06C75;">A</span> = lora_params[b:]  <span style="color: #5C6370; font-style: italic;"># </span><span style="color: #5C6370; font-style: italic;">a x r</span>
<span class="linenr">7: </span>    <span style="color: #C678DD;">return</span> x @ parameter.T + x @ B @ A.T * sigma
<span class="linenr">8: </span>
<span class="linenr">9: </span><span style="color: #E06C75;">batch_forward</span> = jax.vmap(forward, in_axes=(<span style="color: #56B6C2;">None</span>, <span style="color: #56B6C2;">None</span>, <span style="color: #56B6C2;">None</span>, 0, 0))
</pre>
</div>

<p>
In the limit as rank increases<sup><a id="fnr.3" class="footref" href="#fn.3" role="doc-backlink">3</a></sup>, the "gradient" estimate just standard ES with the structured matrix:<br />
</p>

<p>
\[ \nabla_{\theta}\mathbb{E}_{\epsilon_1, \epsilon_2 \sim N(0,I_{d})} F(\theta+\sigma\epsilon_2 \epsilon_1^T) = \frac{1}{\sigma}\mathbb{E}_{\epsilon_1,\epsilon_2\sim N(0,I_{d})}\{F(\theta+\sigma\epsilon_2 \epsilon_1^T)\epsilon_2 \epsilon_1^T\} \]<br />
</p>

<p>
Note that although individual perturbations are low-rank, the expression on the right side is actually high-rank, due to the properties of sums of low-rank matrices. We directly fuse this high rank update into the parameters at each update step, which is why we refer to this as <b>Online Low-rank Learning</b>. In comparison, using ES to directly optimize LoRA matrices will still be restricted to a low-rank perturbation, which may be <a href="https://thinkingmachines.ai/blog/lora/">enough for LLM reasoning</a> but not sufficient for supervised fine-tuning or training from scratch.<br />
</p>
</div>
</div>
<div id="outline-container-orgb5b7297" class="outline-2">
<h2 id="orgb5b7297">Tabula Rasa Environment Learning</h2>
<div class="outline-text-2" id="text-orgb5b7297">
<p>
Evolution Strategies have historically been used as an alternative to reinforcement learning methods, due to its easy scalability on CPU clusters, as done in <a href="https://arxiv.org/abs/1703.03864">OpenAI's Evolution Strategies paper</a>. To demonstrate that the low-rank update does not compromise the performance of ES on these settings, we run experiments on [insert environments here].<br />
</p>
</div>
</div>
<div id="outline-container-org3b16f0d" class="outline-2">
<h2 id="org3b16f0d">LLM Reasoning</h2>
<div class="outline-text-2" id="text-org3b16f0d">
<p>
To highlight the scalability of our method, we do standard LLM "reasoning" training on the RWKV-7 language models.<br />
</p>

<button type="button" class="collapsible" onclick="toggleCollapse(this)">Why RWKV?</button>
<div class="contentx">
<p>
We wanted a language model that is easy to implement in jax without worrying about dynamic state sizes while having high throughput. Transformers are painful to implement in jax due to the growing size of the KV-cache, which would bottleneck the total number of generations we can have in parallel. Recurrent models are more ideal, and since we already built the <a href="https://github.com/bsarkar321/jaxrwkv">jaxrwkv codebase</a> it was trivial to port it to our EGGROLL library.<br />
</p>

<p>
Furthermore, we already have significant experience using RWKV-based models for RL, as we were the first to implement multi-agent, multi-turn learning in a true embodied environment using RWKV in the game of Among Us (see <a href="https://socialdeductionllm.github.io/">here</a>). The same reasons for using RWKV presented in that paper apply here: we want to generate trajectories with constant time and space complexity per token and the latest RWKV7 "Goose" models, which have reasoning traces in their pretraining corpus but otherwise do not do any SFT or RL, are very strong starting points for our reasoning experiments.<br />
</p>

<p>
We are actively working on a vLLM and Megatron port with advisors from NVIDIA, which would enable us to try EGGROLL on other LLMs, including Discrete Diffusion models for which the standard policy gradient theorem is technically intractable (due to the mask-based sampling procedure).<br />
</p>
</div>

<p>
We first test our method on the countdown task to compare against the core results of the concurrent paper on <a href="https://arxiv.org/abs/2509.24372">ES with LLMs</a>. On the RWKV 1.5B model, we outperform the DR GRPO baseline, landing between the reported results of LLaMA-3.2 1B and Qwen 2.5 1.5B:<br />
</p>
<img src="countdown_small.png" style="width:80%;image-rendering:auto;">

<p>
For the 7B model, we outperform all reported results from the other paper<sup><a id="fnr.grpocountdown" class="footref" href="#fn.grpocountdown" role="doc-backlink">4</a></sup>:<br />
</p>
<img src="countdown_large.png" style="width:80%;image-rendering:auto;">

<p>
We also test our model on math benchmarks, namely GSM8K and AIME. [TODO]<br />
</p>
</div>
</div>
<div id="outline-container-orgee37ebe" class="outline-2">
<h2 id="orgee37ebe">Pure Integer Pretraining</h2>
<div class="outline-text-2" id="text-orgee37ebe">
<p>
To highlight the potential of EGGROLL for training beyond gradient-based methods, we built a new language model architecture designed for fast inference that can work across a variety of accelerators. Some of the design decision include:<br />
</p>
<div class="step2bullets">
<ul class="org-ul">
<li>Storing all weights and performing all matrix multiplications in int8, as this is the fastest supported datatype on <a href="https://www.nvidia.com/en-gb/data-center/h100/">H100s</a>.<br /></li>
<li>Never casting to floating point, because keeping all operations in integer formats can enable simpler accelerators. Our model only conducts int8 matrix multiplications (with int32 accumulate) and integer vector addition, multiplication, clipping, and bitwise manipulations. In fact, even our fitness calculation of the log likelihood of generation is done in int32 and precalculated lookup tables. Although prior work like <a href="https://arxiv.org/abs/2310.11453">BitNet</a> does quantization-aware training, we are not aware of prior work to keep the entire training process in int formats.<br /></li>
<li>We use a fully quantized variant of <a href="https://arxiv.org/abs/1701.03452">minGRU</a> instead of self-attention or linear attention to demonstrate that sequence-parallelism is not needed for evolution, just a very large batch size to process individual tokens in parallel. With evolution, we can optimize for any sequence length without increasing memory requirements and take multiple updates within a single sequence using <a href="https://arxiv.org/abs/2304.12180v2">Noise-Reuse ES</a>.<br /></li>
</ul>
</div>

<p>We name our architecture the <b>E</b>volved <b>G</b>enerative <b>G</b>ru, or <b>EGG</b> for short.</p>

<p>
To optimize EGG with EGGROLL, we keep the core infrastructure, but we make slight modifications to the optimization process. In particular, instead of taking arbitrarily large updates, we check if the update is larger than some threshold before pushing it one step in that direction. We do not use any dedicated optimizer states, like momentum, but this would be an interesting future direction.<br />
</p>

<p>
To test our architecture and method, we trained a small 5M parameter model (D256-L6) on a single RTX 3080, achieving over 2.3 million tokens per second. Here is the training curve, where the y axis is bits per byte:<br />
</p>

<img src="pretraining.png" style="width:80%;image-rendering:auto;">

<p>
With 3.2 GB of data<sup><a id="fnr.5" class="footref" href="#fn.5" role="doc-backlink">5</a></sup>, we get a loss of around 4 bits/byte in 25 mins on one RTX 3080. Note that a unigram model would get a loss of 4.91 bits/byte while gzip with max compression gives 2.767 bits/byte. The models are clearly able to learn something, but we still consider this very preliminary and not a practical alternative to gradient-based pretraining <b>yet</b>. However, we are actively scaling up the model to see how far we can get with this crazy architecture.<br />
</p>
</div>
</div>
<div id="outline-container-org4234a44" class="outline-2">
<h2 id="org4234a44"></h2>
<div class="outline-text-2" id="text-org4234a44">
</div><div><div class="footer"><p id="copyright">&copy; 2025 Bidipta Sarkar</p></div></div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Our first public github commit of an early version of EGGROLL is <a href="https://github.com/bsarkar321/jaxrwkv/commit/6d92566eacc2c8c12a946b3e2a3d832dbc1e63fe">here</a>, on Aug 13.<br />
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
The concurrent work on <a href="https://arxiv.org/abs/2509.24372">ES on LLMs</a> avoided this issue by having a very small population size, rolling out hundreds of samples for each member of the population to get a better fitness estimate and running a small number of population members on the GPU at any one time, turning the batched matrix multiplication back into regular matrix multiplication. However, we would like to have the flexibility to choose between having multiple rollouts per population member versus having more population members overall.<br />
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3" role="doc-backlink">3</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
The theory claims that this is the gradient estimate at the limit as rank increases, and we empirically find that this continues to be strong even at rank 1.<br />
</p></div></div>

<div class="footdef"><sup><a id="fn.grpocountdown" class="footnum" href="#fnr.grpocountdown" role="doc-backlink">4</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Our GRPO training is too slow to get reasonable results for the 7B RWKV model, since we have not incorporated cuda kernels yet. However, this is on the roadmap to give a fair comparison.<br />
</p></div></div>

<div class="footdef"><sup><a id="fn.5" class="footnum" href="#fnr.5" role="doc-backlink">5</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
The amount of unique data is actually 1.6 GB due to antithetical sampling.<br />
</p></div></div>


</div>
</div></div>
</body>
</html>
